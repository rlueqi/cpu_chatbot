# ì°¸ê³ : https://docs.streamlit.io/develop/tutorials/chat-and-llm-apps/build-conversational-apps

from openai import OpenAI
import streamlit as st
import os

prompt = """
ë‹¹ì‹ ì€ ëª¨ë“  ì–¸ì–´ì— í†µë‹¬í•œ ì–¸ì–´ ì²œì¬ì…ë‹ˆë‹¤. ëŒ€í™”ë¥¼ í•  ë•Œ íŠ¹ì • ì–¸ì–´ì—
ëŒ€í•œ ì£¼ì œê°€ ë‚˜ì˜¤ë©´ ê·¸ ì–¸ì–´ì— ëŒ€í•´ ìì„¸íˆ ì•Œë ¤ì£¼ê³  ì˜ëª»ëœ í‘œí˜„ì„
ê³ ì³ì£¼ë ¤ê³  í•©ë‹ˆë‹¤.

í‘œí˜„ ìˆ˜ì •:
ì˜ëª»ëœ í‘œí˜„ì„ ì“°ê±°ë‚˜ ë” í‰ë²”í•˜ê²Œ ì“¸ ìˆ˜ ìˆëŠ” ë§ì´ ìˆëŠ” ê²½ìš°
1. "ê·¸ í‘œí˜„ë³´ë‹¤ ë” ì¢‹ì€ í‘œí˜„ì´ ìˆì–´"ë¼ê³  ë°˜ì‘
2. ê³§ì´ì–´ "ë‚˜ë¼ë©´ ì´ëŸ° í‘œí˜„ì„ ì“¸ê±°ì•¼" ëŒ€ë‹µ
3. ì ì ˆí•œ í‘œí˜„ì— ëŒ€í•´ ì„¤ëª… ë° ë¬¸í™”ì  ë°°ê²½, ì—­ì‚¬ì  ë°°ê²½ ë“± ì„¤ëª…
4. í‘œí˜„ì´ ì´í•´ê°€ ë˜ì—ˆëŠ”ì§€ ì§ˆë¬¸
5. ë‹¤ìŒì—ëŠ” ì–´ë–¤ ê²ƒì´ ê¶ê¸ˆí•œì§€ ë¬»ê¸°

ì§ˆë¬¸ ìŠ¤íƒ€ì¼:
- "XXë¼ëŠ” í‘œí˜„ì€ ì´ëŸ¬í•œ XXì  ë°°ê²½ì´ ìˆì–´ì„œì•¼"
- "ì´ê²ƒê³¼ ë¹„ìŠ·í•œ í‘œí˜„ì€ XXê°€ ìˆì–´!"
- "ì½œë¡œì¼€ì´ì…˜ê³¼ êµ¬ë™ì‚¬ë¥¼ ì“°ëŠ” ì´ìœ ëŠ” XXì•¼"
- "ë˜ ê¶ê¸ˆí•œ ë‚´ìš©ì´ ìˆì–´"

ê·œì¹™:
- í˜„ì¬ ì“°ëŠ” í‘œí˜„ë§Œ ì‚¬ìš©í•˜ê¸°
- ì •ë³´ê°€ ë¶€ì¡±í•´ì„œ ëª» ì•Œë ¤ì£¼ëŠ” ì–¸ì–´ëŠ” "ìë£Œ ë¶€ì¡±"ìœ¼ë¡œ í‘œì‹œ
- ì§ˆë¬¸ì„ í•  ë•Œ ì‚¬ìš©í•˜ëŠ” ì–¸ì–´ë¡œ ëŒ€ë‹µí•˜ê¸°
- í•œêµ­ì–´ ë°œìŒê³¼ êµ­ì œìŒì„±ê¸°í˜¸ë¥¼ ê°™ì´ ë„£ê¸°
"""

# Cerebras APIë¥¼ ì‚¬ìš©í•˜ì—¬ OpenAI API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
client = OpenAI(
    base_url="https://api.cerebras.ai/v1",
    api_key=os.getenv("CEREBRAS_API_KEY")
)

# Cerebras ëª¨ë¸ ì‚¬ìš©
# https://inference-docs.cerebras.ai/models/overview
# "qwen-3-32b"
# "qwen-3-235b-a22b-instruct-2507",
# "qwen-3-coder-480b"
# "llama-4-scout-17b-16e-instruct"
# "qwen-3-235b-a22b-thinking-2507"
# "llama-3.3-70b"
# "llama3.1-8b"
# "gpt-oss-120b"
llm_model = "gpt-oss-120b"  
if "llm_model" not in st.session_state:
    st.session_state["llm_model"] = llm_model

st.title("ì–¸ì–´ ìŠµë“ìš© íŠ¹í™” AI")

# ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì„¤ì •
if "messages" not in st.session_state:
    st.session_state.messages = [
        {
            "role": "system", 
            "content": prompt,
        }
    ]

avatars = {"user": "ğŸ¤”", "assistant": "ğŸŒ"}

for message in st.session_state.messages:
    if message["role"] == "system":
        continue
    with st.chat_message(message["role"], avatar=avatars.get(message["role"])):
        st.markdown(message["content"])

if prompt := st.chat_input("ì–¸ì–´ì™€ ê´€ë ¨ëœ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user", avatar=avatars["user"]):
        st.markdown(prompt)

    with st.chat_message("assistant", avatar=avatars["assistant"]):
        # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ë°›ê¸°
        stream = client.chat.completions.create(
            model=st.session_state["llm_model"],
            messages=[
                {"role": m["role"], "content": m["content"]}
                for m in st.session_state.messages
            ],
            temperature=0.7,
            max_completion_tokens=1000,
            stream=True
        )
        response = st.write_stream(stream)
    st.session_state.messages.append({"role": "assistant", "content": response})


if __name__ == "__main__":
    import subprocess
    import sys
    
    # í™˜ê²½ ë³€ìˆ˜ë¡œ ì¬ì‹¤í–‰ ë°©ì§€
    if not os.environ.get("STREAMLIT_RUNNING"):
        os.environ["STREAMLIT_RUNNING"] = "1"
        subprocess.run([sys.executable, "-m", "streamlit", "run", __file__])

# python -m streamlit run main.py
# streamlit run main.py